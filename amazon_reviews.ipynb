{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source: https://www.kaggle.com/datasets/yasserh/amazon-product-reviews-dataset\n",
    "# Folder: Amazon\n",
    "# Description:\n",
    "### The dataset consists of samples from Amazon Ratings for select products.\n",
    "### The reviews are picked randomly and the corpus has nearly 1.6k reviews of different customers.\n",
    "### Amazon aims to understand what are the main topics of these reviews to classify them for easier search.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, Analysis, Visualization, and Modeling of Amazon Product Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "- Understand the Dataset & perform the necessary cleanup.\n",
    "- Add additional algorithms to go in depth on the positivity of each review\n",
    "- Build a strong Topic Modelling Algorithm to classify the topics a bit more than what is provided in each review's title.\n",
    "- Create a regression model to predict product ratings based on the length of reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Tools used throughout\n",
    "- Pandas (data cleaning and manipulation)\n",
    "- NLTK & spaCy (NLP)\n",
    "- sklearn (regression)\n",
    "- langdetect & googletrans (detecting non-english languages and translating to english)\n",
    "- Gensim (topic modelling)\n",
    "- pyLDAvis & matplotlib (visualizing topic model)\n",
    "- warnings (prevent certain warnings from showing up and or displaying personal information on user's device after execution of a cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the case of errors:\n",
    "- Not all python libraries may be on your machine and or within your directory. Ensure to install them.\n",
    "    - They may not be updated to a version for certain operations to occur. Update them to latest versions.\n",
    "- You ran a cell with a problematic edit that you made to it(This notebook is designed to run seamlessly with no edits)\n",
    "- Not running a python kernel or you're using an old version of python kernel\n",
    "- Don't have libraries or necessary downloads that are necessary for operation of parts or the entirety certain libraries.\n",
    "    - ex. vader_lexicon is required to be downloaded with Sentiment Analysis(later on in the notebook)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "from sklearn.linear_model import LinearRegression  # minimum model to be used later.\n",
    "import nltk # for NLP\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\") # ignore all overall\n",
    "warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning) # ignore a warning later on for copying over on a dataframe.\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the file and create dataframe\n",
    "df = pd.read_csv('product_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this we can see that this Dataset contains a lot of columns. For the purpose of our analyses, we only need a few"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For reference, here is a description of each column \n",
    "\n",
    "- **id:** Unique identifier for each product.\n",
    "- **asins:** ASIN (Amazon Standard Identification Number) associated with the product.\n",
    "- **brand:** Brand of the product.\n",
    "- **categories:** Categories to which the product belongs.\n",
    "- **colors:** Colors available for the product.\n",
    "- **dateAdded:** Date when the product was added.\n",
    "- **dateUpdated:** Date when the product information was last updated.\n",
    "- **dimension:** Dimensions of the product.\n",
    "- **ean:** EAN (European Article Number) associated with the product.\n",
    "- **keys:** Unique keys associated with the product.\n",
    "- **manufacturer:** Manufacturer of the product.\n",
    "- **manufacturerNumber:** Manufacturer number for the product.\n",
    "- **name:** Name of the product.\n",
    "- **prices:** Prices associated with the product, including currency and date information.\n",
    "- **reviews.date:** Date when the review was posted.\n",
    "- **reviews.doRecommend:** Indicates whether the reviewer recommends the product.\n",
    "- **reviews.numHelpful:** Number of users who found the review helpful.\n",
    "- **reviews.rating:** Rating given by the reviewer.\n",
    "- **reviews.sourceURLs:** URLs to the source of the reviews.\n",
    "- **reviews.text:** Text content of the review.\n",
    "- **reviews.title:** Title of the review.\n",
    "- **reviews.userCity:** City of the reviewer.\n",
    "- **reviews.userProvince:** Province of the reviewer.\n",
    "- **reviews.username:** Username of the reviewer.\n",
    "- **sizes:** Sizes available for the product.\n",
    "- **upc:** UPC (Universal Product Code) associated with the product.\n",
    "- **weight:** Weight of the product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get an easier idea of all the columns we are working with, let us see how many exist\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a new df including more of what is actually relevant\n",
    "relevant_columns = ['id', 'asins', 'brand', 'categories', 'colors', 'manufacturer',\n",
    "        'name', 'prices', 'reviews.date',\n",
    "       'reviews.doRecommend', 'reviews.numHelpful', 'reviews.rating', 'reviews.text', 'reviews.title',\n",
    "         'sizes', 'weight']\n",
    "product_reviews = df[relevant_columns]\n",
    "product_reviews.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have a dataset with more of the information we need, we have spotted that a few columns needs restructuring\n",
    "### Specifically the prices column and the reviews date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews['prices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews['reviews.date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change format to datetime\n",
    "product_reviews['reviews.date'] = pd.to_datetime(product_reviews['reviews.date'], format='ISO8601')\n",
    "\n",
    "# Gets rid of milliseconds\n",
    "product_reviews['reviews.date'] = product_reviews['reviews.date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "product_reviews['reviews.date'].dtype #still datetime but is stored as object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews['reviews.date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test to make sure things are working as intended\n",
    "product_reviews['reviews.date'] > '2016-02-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the date is fixed, we will move on to fixing the price column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a refresher here are what values in the price column look like\n",
    "prices_first_row = product_reviews['prices'][0]\n",
    "print(prices_first_row)\n",
    "print(type(prices_first_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is a lot to take in so we'll adjust it to be more presentable\n",
    "import json\n",
    "\n",
    "# convert the value that is currently a str to a list with dictionaries\n",
    "prices_1 = json.loads(prices_first_row)\n",
    "print(\"before proper formatting; \", type(prices_1))\n",
    "\n",
    "# makes it more presentable within json format\n",
    "prices_1_format = json.dumps(prices_1, indent = 3)\n",
    "print(prices_1_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For our purposes, we only want prices in USD. With the example shown above we see that there can be multiple prices in USD\n",
    "- The original price when not on sale and the sale price.\n",
    "\n",
    "## With this knowledge, we'll create two extra columns to the product reviews table and store those prices in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure all columns have a price in USD\n",
    "len(product_reviews['prices'].str.contains(\"USD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store OG price and sale price takes in the prices in USD for each item\n",
    "full_prices = []\n",
    "sale_prices = []\n",
    "\n",
    "for i in product_reviews.index:\n",
    "    list_dict = json.loads(product_reviews['prices'][i])\n",
    "\n",
    "    # Initialize variables to store original and sale prices\n",
    "    original_price = float(list_dict[0]['amountMax'])\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate through the list of dictionaries to find prices\n",
    "    for price_info in list_dict:\n",
    "        if price_info.get('currency') == 'USD' and price_info.get('isSale') == 'true':\n",
    "            sale_price = float(price_info['amountMax'])\n",
    "            break\n",
    "\n",
    "\n",
    "    # Append prices to respective lists\n",
    "    full_prices.append(original_price)\n",
    "    sale_prices.append(sale_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to ensure if the loop above needs to be adjusted to include a substitute value if there isnt a sale price\n",
    "print(len(sale_prices),len(full_prices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add two columns to showcase the two prices\n",
    "product_reviews.insert(8,'fullPrice',full_prices)\n",
    "product_reviews.insert(9,'salePrice',sale_prices)\n",
    "product_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that this is done, we no longer need the original price column\n",
    "product_reviews = product_reviews.drop(columns='prices')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data is finally clean and we will now move on to utilizing NLP for the following purposes\n",
    "- elaborating on how positive each review is\n",
    "    - creating a classification model to then support classifying the level of positivity\n",
    "- topic of each review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for an intro to the natural language processing toolkit and the different language packages it has. Close it when you've had a good view of the GUI\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon',quiet=True) # required to be used with sentiment analysis intensity\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for identifying the level of sentiment(neg to pos) of text\n",
    "\n",
    "# class and function of sentiment intensity analysis\n",
    "sia = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check to make sure all products have reviews.\n",
    "product_reviews['reviews.text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for translation\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores_data = []\n",
    "\n",
    "for review in product_reviews['reviews.text']:\n",
    "    # Check if the review is in English\n",
    "    try:\n",
    "        if detect(review) != 'en':\n",
    "            # Translate non-English reviews to English\n",
    "            translation = translator.translate(review, dest='en').text\n",
    "            review = translation\n",
    "\n",
    "        # Analyze sentiment for the (translated or original) review\n",
    "        score = sia.polarity_scores(review)\n",
    "        scores_data.append(score)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing review: {e}\")\n",
    "\n",
    "scores_data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a column to store the positivity scores\n",
    "product_reviews.insert(15,'positivityScore',[scores_data[i]['compound'] for i in range(len(scores_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing proper labels for each review in a list\n",
    "positivity_level = []\n",
    "\n",
    "for i in product_reviews['positivityScore']:\n",
    "    if .66 <= i <= 1:\n",
    "        positivity_level.append(\"highly positive\")\n",
    "    elif .33 <= i < .66:\n",
    "        positivity_level.append(\"positive\")\n",
    "    elif .1 <= i < .33:\n",
    "        positivity_level.append(\"fairly positive\")\n",
    "    elif -.1 <= i < .1:\n",
    "        positivity_level.append(\"neutral\")\n",
    "    elif -.33 <= i < -.1:\n",
    "        positivity_level.append(\"fairly negative\")\n",
    "    elif -.66 <= i < -.33:\n",
    "        positivity_level.append(\"negative\")\n",
    "    elif -1 <= i < -.66:\n",
    "        positivity_level.append(\"highly negative\")\n",
    "\n",
    "\n",
    "# inserting the values from the list into a column for positivity level\n",
    "product_reviews.insert(16,'positivityLevel',positivity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we'll go over to creating the algorithm for identifying the main topics reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for splitting the reviews by words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# english tokenizer that adds more depth to the tokenizer\n",
    "nltk.download(\"punkt\",quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating the model and additional tools to assist or support it\n",
    "# Topic modeling library\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# visualizing the model\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# storing stopwords in a variable to be used pretty soon\n",
    "stopwords = stopwords.words(\"english\")\n",
    "stopwords[0:100:10] # examples of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "# Data to be used in the Topic modeling algorithm\n",
    "data = product_reviews['reviews.text']\n",
    "\n",
    "for i in range(len(data)):\n",
    "    review = data[i]\n",
    "\n",
    "    # Check if the review is in English\n",
    "    try:\n",
    "        if detect(review) != 'en':\n",
    "            # Translate non-English reviews to English\n",
    "            translation = translator.translate(review, dest='en').text\n",
    "            data[i] = translation\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing review: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break words down to their most basic form to allow for the model used later to create a better model\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "\n",
    "    # for loading in the data and also applying tokenization and other language processing to it in respects to the english language\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "\n",
    "            # eliminates stopwords\n",
    "            if token.pos_ in allowed_postags and token.text.lower() not in stopwords:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(data)\n",
    "print (lemmatized_texts[0][0:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further pre-processing of the texts\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "\n",
    "        # tokenizing the words and getting rid of accent marks on words if there are\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "print (data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate bigrams and trigrams if they potentially exist by checking if two or three words appear by each other enough times that they are probably meant to be used together\n",
    "bigrams_phrases = gensim.models.Phrases(data_words, min_count = 5, threshold = 50)\n",
    "trigram_phrases = gensim.models.Phrases(bigrams_phrases[data_words], threshold = 50)\n",
    "\n",
    "# turning phrases into a Phraser object, which can then be used to apply those phrases to new sentences\n",
    "bigram = gensim.models.phrases.Phraser(bigrams_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "print(data_bigrams_trigrams[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary mapping words to unique IDs\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Creating a bag-of-words representation of the corpus\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    # Converting each document to a bag-of-words format\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "# Printing the bag-of-words representation of the first document (up to the first 20 elements)\n",
    "print(corpus[0][0:20])\n",
    "\n",
    "# Retrieving the word corresponding to the first unique ID in the dictionary\n",
    "word = id2word[[0][:1][0]]\n",
    "\n",
    "print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and cleaning the texts is complete. Now we go on to creating the model(s) and doing some analysis on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example hyperparameter tuning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vary the number of topics\n",
    "num_topics_list = list(range(3, 16))\n",
    "coherence_scores = []\n",
    "\n",
    "#creates a model for each number of topics in the range and then produces a coherence model to see how well words in the clusters are connected to each other in making a topic\n",
    "for num_topics in num_topics_list:\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha=\"auto\"\n",
    "    )\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "# Plot the coherence scores, highest is the best\n",
    "plt.plot(num_topics_list, coherence_scores)\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('Coherence Score vs. Number of Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=6, # play around with. Coherence model helps guide to the best number for this, which are 6, 8, and 10\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the different clusters of topics. \n",
    "- This is unsupervised so it is up to the user to determine the labeling of the actual clusters(topic for each cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the model & details of it. Interactive display(must be run on personal machine as it is not displayable in github)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main topics throughout all reviews after looking at most frequent words in each cluster\n",
    "main_topics = [\n",
    "    \"Device and Display Experience\",\n",
    "    \"Audio and Speaker Performance\",\n",
    "    \"Headphones and Sound Quality\",\n",
    "    \"Content Consumption and App/Software Experience\",\n",
    "    \"General Product Review and Comparison\",\n",
    "    \"TV Box and Streaming Experience\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On top of analyzing the texts a lot for its meaning. Let's also check to see if any trends can be attached to them as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifically, we want to see if there is a decent trend between the number of people who found a review helpful and the actual length of the review\n",
    "- If there is an identifiable trend, it would be good to create a model for predictions based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we want to see if there are any null values between both fields\n",
    "product_reviews[['reviews.text','reviews.numHelpful']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To fix this, we'll substitute the null values for 0\n",
    "product_reviews['reviews.numHelpful'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews[['reviews.text','reviews.numHelpful']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To check on the trend, I'll create a scatter plot\n",
    "X = product_reviews['reviews.text'].apply(len).to_frame()\n",
    "y = product_reviews['reviews.numHelpful']\n",
    "\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Characters in review\")\n",
    "plt.ylabel(\"helpful score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From this we can see that there is definitely a relationship and one that\n",
    "## Further this relationship seems to be best fitting for a polynomial model\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# creating the test and training datasets for the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create the training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.33, random_state = 26)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing that standardizes the dataset applied to it\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# the mean and std is computed for each feature in X_train and then the standardization ((x - u)/std) is applied\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "\n",
    "# same is applied here, but we dont apply fit, because we want it to use the mean and std from the X_train_scaler\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "#linear\n",
    "lin = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is really a linear model but due to the degree being 1.\n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "\n",
    "# applies the same fit and transform from last cell but specifically for the X_train_scaler\n",
    "X_poly_train = poly.fit_transform(X_train_scaler)\n",
    "\n",
    "# want to use the mean and std gained from X_poly_train and then apply transformation/standardization to it\n",
    "X_poly_test = poly.transform(X_test_scaler)\n",
    "\n",
    "# here we fit the transformed x_poly_train with y_train to then get actual values that minimize the difference between predicted and actual y_train value\n",
    "poly.fit(X_poly_train,y_train)\n",
    "\n",
    "# same idea here given but on linear model to also get minimized values to match y train to x poly train\n",
    "lin.fit(X_poly_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lin model is now trained predicted y values are created in relation to X_poly_test\n",
    "y_pred = lin.predict(X_poly_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# using mse because there aren't any outliers in the values of y as seen in the matplotlib vis.\n",
    "# So its safe to use MSE which is always more accurate as long as outliers arent present. If outliers are present, switch to MAE\n",
    "test_mse = mean_squared_error(y_test,y_pred)\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test_mse is actually very close to the training mse which is most accurate, given the y_training values were seen by the y_pred_training model\n",
    "y_pred_train = lin.predict(X_poly_train)\n",
    "train_mse = mean_squared_error(y_train,y_pred_train)\n",
    "train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the difference between the two to know how well the model is with test_data it hasn't seen\n",
    "print(test_mse - train_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re run with different degree and compare the difference between MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 3)\n",
    "X_poly_train = poly.fit_transform(X_train_scaler)\n",
    "X_poly_test = poly.transform(X_test_scaler)\n",
    "poly.fit(X_poly_train,y_train)\n",
    "lin.fit(X_poly_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin.predict(X_poly_test)\n",
    "test_mse1 = mean_squared_error(y_test,y_pred)\n",
    "test_mse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lin.predict(X_poly_train)\n",
    "train_mse1 = mean_squared_error(y_train,y_pred_train)\n",
    "train_mse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_mse1 - train_mse1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall comparison of both models. Lowest is of the absolute value of both is the best\n",
    "print(\"Differences between MSEs. Degree 2: \",(test_mse - train_mse), \" vs. Degree 3: \",(test_mse1 - train_mse1))\n",
    "print(\"Differences between actual values of training predictions and test predictions respectively\\nDegree 2: \",\\\n",
    "      \"trained -\", train_mse, \" test -\", test_mse,\"\\nDegree 3: \",\"trained -\", train_mse1, \" test -\", test_mse1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the comparison, a polynomial model of degree 3 is the best option to use for the following reasons.\n",
    "- The test and training MSE are lower for the model with degree = 3.\n",
    "- The model with degree = 3 results has smaller difference between the test and training MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall, this marks the end of my project.\n",
    "### Topics showcased include:\n",
    "- Topic Modeling with the Gensim library and additional tools\n",
    "- Sentiment Analysis with nltk\n",
    "- Regression Modeling with sklearn\n",
    "- Lil bits of visualization with matplotlib and a LDA model based visualization library(pyLDAvis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
